{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A2T GPT Model for Travel Domain Transfer Learning\n",
        "\n",
        "This notebook implements a GPT-based model with the A2T (Attend, Adapt, and Transfer) framework for transfer learning from a generic dataset (Tiny Shakespeare) to a travel domain. It incorporates reinforcement learning algorithms: REINFORCE, Actor-Critic, and Q-learning with DQN. The model generates travel itineraries by adapting knowledge from pre-trained source models.\n",
        "\n",
        "## Setup Instructions\n",
        "- Run in Google Colab with a GPU for faster training.\n",
        "- Install required libraries: `!pip install torch transformers tqdm`.\n",
        "- The notebook downloads `input.txt` (Tiny Shakespeare) and creates a sample `travel.txt`. Replace `travel.txt` with a larger dataset for better results.\n",
        "- Choose an RL algorithm (`train_reinforce`, `train_actor_critic`, or `train_dqn`) to train the model.\n",
        "- After training, generate travel itineraries using the prompt 'Plan a trip to Paris'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\THINKPAD\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 2000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "vocab_size = None\n",
        "n_source_tasks = 2\n",
        "replay_buffer_size = 10000\n",
        "epsilon = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "\n",
        "# Download Tiny Shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "shakespeare_text = response.text\n",
        "\n",
        "# Save Tiny Shakespeare dataset to input.txt\n",
        "with open('input.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(shakespeare_text)\n",
        "\n",
        "# Larger travel dataset\n",
        "travel_text = \"\"\"Explore Paris, France:\\nDay 1: Visit the Eiffel Tower, enjoy a Seine River cruise, and dine at a charming café in Montmartre. Don't miss the Sacré-Cœur Basilica for stunning city views.\\nDay 2: Explore the Louvre Museum, home to the Mona Lisa and thousands of artworks. Stroll along the Champs-Élysées and visit the Arc de Triomphe.\\nDay 3: Take a day trip to Versailles Palace, known for its opulent gardens and Hall of Mirrors. Return to Paris for a cozy dinner in Le Marais.\\n\\nDiscover Tokyo, Japan:\\nDay 1: Experience the vibrant Shibuya Crossing and visit the Meiji Shrine. Enjoy sushi at a local restaurant in Ginza.\\nDay 2: Explore Asakusa and the historic Senso-ji Temple. Take a trip to Tokyo Skytree for panoramic views of the city.\\nDay 3: Visit Akihabara for electronics and anime culture, then relax in Ueno Park and explore its museums.\\n\\nPlan a Trip to New York City, NY, USA:\\nDay 1: Start at Times Square, visit the Empire State Building, and catch a Broadway show in the evening.\\nDay 2: Walk through Central Park, visit the Metropolitan Museum of Art, and take a ferry to the Statue of Liberty.\\nDay 3: Explore Brooklyn, including the Brooklyn Bridge and DUMBO neighborhood. Enjoy pizza at a local pizzeria.\\n\\nTravel Tips for Europe:\\n- Book train tickets in advance for Eurail passes to save money.\\n- Pack light but include comfortable walking shoes for cobblestone streets.\\n- Learn basic phrases in the local language to enhance your experience.\\n- Always carry a reusable water bottle to stay hydrated.\\n\\nTravel Tips for Asia:\\n- Respect local customs, such as removing shoes before entering temples.\\n- Try street food but ensure it’s from reputable vendors to avoid foodborne illnesses.\\n- Use apps like Google Translate for real-time translation in non-English-speaking countries.\\n\\nTop Beaches in the World:\\n- Maldives: Crystal-clear waters and overwater bungalows make it a paradise.\\n- Bora Bora, French Polynesia: Stunning lagoons and luxurious resorts.\\n- Santorini, Greece: Black sand beaches and breathtaking caldera views.\\n\\nAdventure Travel in South America:\\n- Hike the Inca Trail to Machu Picchu, Peru, for a bucket-list experience.\\n- Explore the Amazon Rainforest in Brazil for unique wildlife encounters.\\n- Visit Patagonia in Chile for glacier trekking and stunning landscapes.\\n\\nCruise Itineraries:\\n- Mediterranean Cruise: Visit Barcelona, Rome, and Athens for a mix of history and culture.\\n- Caribbean Cruise: Stop at Jamaica, the Bahamas, and Cozumel for sun and fun.\\n- Alaskan Cruise: See glaciers, whales, and fjords in a pristine natural setting.\"\"\"\n",
        "with open('travel.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(travel_text)\n",
        "\n",
        "# Load datasets\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    generic_text = f.read()\n",
        "with open('travel.txt', 'r', encoding='utf-8') as f:\n",
        "    travel_text = f.read()\n",
        "\n",
        "# Create unified vocabulary\n",
        "chars = sorted(list(set(generic_text + travel_text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Encode datasets\n",
        "generic_data = torch.tensor(encode(generic_text), dtype=torch.long)\n",
        "travel_data = torch.tensor(encode(travel_text), dtype=torch.long)\n",
        "\n",
        "# Split data\n",
        "n = int(0.9 * len(travel_data))\n",
        "train_data = travel_data[:n]\n",
        "val_data = travel_data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading\n",
        "def get_batch(split, data_type='travel'):\n",
        "    data = train_data if split == 'train' else val_data if data_type == 'travel' else generic_data\n",
        "    if len(data) < block_size + 1:\n",
        "        raise ValueError(f'Dataset too small for block_size={block_size}. Reduce block_size or expand dataset.')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "# Loss estimation\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, data_type='travel'):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            try:\n",
        "                X, Y = get_batch(split, data_type)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            except ValueError:\n",
        "                losses[k] = float('inf')\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanse action function\n",
        "def cleanse_action(action_text):\n",
        "    return action_text.strip()\n",
        "\n",
        "# Simulated travel environment\n",
        "class TravelEnvironment:\n",
        "    def __init__(self):\n",
        "        self.sentiment_pipe = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', device=0 if device == 'cuda' else -1)\n",
        "    \n",
        "    def step(self, state, action):\n",
        "        state_text = decode(state.tolist())\n",
        "        action_text = cleanse_action(decode([action.item()]))\n",
        "        response = state_text + action_text\n",
        "        reward = self.sentiment_pipe(response)[0]['score'] if self.sentiment_pipe(response)[0]['label'] == 'POSITIVE' else -self.sentiment_pipe(response)[0]['score']\n",
        "        next_state = torch.cat((state[1:], torch.tensor([action], device=device)), dim=0)\n",
        "        done = len(state_text) >= block_size\n",
        "        return next_state, reward, done\n",
        "\n",
        "# Replay buffer for DQN\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return torch.stack(state), torch.stack(action), torch.tensor(reward, device=device), torch.stack(next_state), torch.tensor(done, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attention Network\n",
        "class AttentionNetwork(nn.Module):\n",
        "    def __init__(self, n_embd, n_source_tasks):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_source_tasks + 1),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Transformer Components\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bigram Language Model for source tasks\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        logits = self.lm_head(tok_emb)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# A2T Model\n",
        "class A2TModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_source_tasks):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.q_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.attention_network = AttentionNetwork(n_embd, n_source_tasks)\n",
        "        self.source_models = nn.ModuleList([BigramLanguageModel() for _ in range(n_source_tasks)])\n",
        "    \n",
        "    def forward(self, idx, targets=None, mode='policy'):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        \n",
        "        if mode == 'policy':\n",
        "            base_logits = self.lm_head(x)\n",
        "        else:\n",
        "            base_logits = self.q_head(x)\n",
        "        \n",
        "        source_logits = [model(idx)[0] for model in self.source_models]\n",
        "        attention_input = x[:, -1, :]\n",
        "        weights = self.attention_network(attention_input)\n",
        "        combined_logits = weights[:, -1:].unsqueeze(-1) * base_logits\n",
        "        for i in range(n_source_tasks):\n",
        "            combined_logits += weights[:, i:i+1].unsqueeze(-1) * source_logits[i]\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = combined_logits.shape\n",
        "            logits = combined_logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        \n",
        "        return combined_logits, loss, weights\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, mode='policy'):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _, _ = self(idx_cond, mode=mode)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source model 1 trained, final loss: 2.3680\n",
            "Source model 2 trained, final loss: 2.3633\n"
          ]
        }
      ],
      "source": [
        "# Initialize environment and replay buffer\n",
        "env = TravelEnvironment()\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "\n",
        "# Initialize model and optimizers\n",
        "model = A2TModel(vocab_size, n_source_tasks).to(device)\n",
        "optimizer_policy = torch.optim.AdamW(list(model.lm_head.parameters()) + list(model.attention_network.parameters()), lr=learning_rate)\n",
        "optimizer_value = torch.optim.RMSprop(list(model.q_head.parameters()) + list(model.attention_network.parameters()), lr=learning_rate)\n",
        "\n",
        "# Pre-train source models on generic dataset\n",
        "for i, source_model in enumerate(model.source_models):\n",
        "    source_optimizer = torch.optim.AdamW(source_model.parameters(), lr=learning_rate)\n",
        "    for iter in range(1000):\n",
        "        xb, yb = get_batch('train', data_type='generic')\n",
        "        logits, loss = source_model(xb, yb)\n",
        "        source_optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        source_optimizer.step()\n",
        "    print(f'Source model {i+1} trained, final loss: {loss.item():.4f}')\n",
        "\n",
        "# Training loop for A2T with REINFORCE\n",
        "def train_reinforce():\n",
        "    model.train()\n",
        "    for iter in tqdm(range(max_iters)):\n",
        "        try:\n",
        "            xb, yb = get_batch('train')\n",
        "            state = xb[:, :-1]\n",
        "            action = xb[:, -1]\n",
        "            \n",
        "            logits, loss, weights = model(state, mode='policy')\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            action_dist = torch.distributions.Categorical(probs)\n",
        "            sampled_action = action_dist.sample()\n",
        "            next_state, reward, done = env.step(state[0], sampled_action[0])\n",
        "            \n",
        "            log_prob = action_dist.log_prob(sampled_action)\n",
        "            policy_loss = -log_prob * reward\n",
        "            optimizer_policy.zero_grad()\n",
        "            policy_loss.mean().backward()\n",
        "            optimizer_policy.step()\n",
        "            \n",
        "            if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "                losses = estimate_loss(model)\n",
        "                print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
        "        except ValueError as e:\n",
        "            print(f'Error at iteration {iter}: {e}')\n",
        "            break\n",
        "\n",
        "# Training loop for A2T with Actor-Critic\n",
        "def train_actor_critic():\n",
        "    model.train()\n",
        "    critic = nn.Sequential(\n",
        "        nn.Linear(n_embd, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 1)\n",
        "    ).to(device)\n",
        "    critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=learning_rate)\n",
        "    \n",
        "    for iter in tqdm(range(max_iters)):\n",
        "        try:\n",
        "            xb, yb = get_batch('train')\n",
        "            state = xb[:, :-1]\n",
        "            logits, _, weights = model(state, mode='policy')\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            action_dist = torch.distributions.Categorical(probs)\n",
        "            action = action_dist.sample()\n",
        "            next_state, reward, done = env.step(state[0], action[0])\n",
        "            \n",
        "            state_emb = model.blocks(model.token_embedding_table(state) + model.position_embedding_table(torch.arange(state.shape[1], device=device)))\n",
        "            value = critic(state_emb[:, -1, :])\n",
        "            next_state_emb = model.blocks(model.token_embedding_table(next_state.unsqueeze(0)) + model.position_embedding_table(torch.arange(next_state.shape[0], device=device)))\n",
        "            next_value = critic(next_state_emb[:, -1, :])\n",
        "            \n",
        "            delta = reward + (0.99 * next_value * (1 - done)) - value\n",
        "            critic_loss = delta ** 2\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optimizer.step()\n",
        "            \n",
        "            log_prob = action_dist.log_prob(action)\n",
        "            actor_loss = -log_prob * delta.detach()\n",
        "            optimizer_policy.zero_grad()\n",
        "            actor_loss.mean().backward()\n",
        "            optimizer_policy.step()\n",
        "            \n",
        "            if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "                losses = estimate_loss(model)\n",
        "                print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
        "        except ValueError as e:\n",
        "            print(f'Error at iteration {iter}: {e}')\n",
        "            break\n",
        "\n",
        "# Training loop for A2T with Q-learning (DQN)\n",
        "def train_dqn():\n",
        "    model.train()\n",
        "    target_model = A2TModel(vocab_size, n_source_tasks).to(device)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "    replay_buffer.clear()\n",
        "    \n",
        "    for iter in tqdm(range(max_iters)):\n",
        "        try:\n",
        "            xb, yb = get_batch('train')\n",
        "            state = xb[:, :-1]\n",
        "            if random.random() < epsilon:\n",
        "                action = torch.randint(0, vocab_size, (batch_size,), device=device)\n",
        "            else:\n",
        "                q_values, _, _ = model(state, mode='value')\n",
        "                action = q_values[:, -1, :].argmax(dim=-1)\n",
        "            next_state, reward, done = env.step(state[0], action[0])\n",
        "            replay_buffer.push(state[0], action[0], reward, next_state, done)\n",
        "            \n",
        "            if len(replay_buffer.buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "                q_values, _, _ = model(states, mode='value')\n",
        "                q_values = q_values.gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "                with torch.no_grad():\n",
        "                    next_q_values, _, _ = target_model(next_states, mode='value')\n",
        "                    target_q = rewards + (1 - dones) * 0.99 * next_q_values.max(dim=-1)[0]\n",
        "                loss = F.mse_loss(q_values, target_q)\n",
        "                optimizer_value.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer_value.step()\n",
        "            \n",
        "            if iter % 100 == 0:\n",
        "                target_model.load_state_dict(model.state_dict())\n",
        "            \n",
        "            if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "                losses = estimate_loss(model)\n",
        "                print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
        "        except ValueError as e:\n",
        "            print(f'Error at iteration {iter}: {e}')\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/2000 [00:03<34:57,  1.05s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 103/2000 [00:15<13:33,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 203/2000 [00:26<12:50,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 200: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 303/2000 [00:38<12:18,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 300: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 403/2000 [00:48<10:53,  2.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 400: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 503/2000 [00:59<10:25,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 500: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 603/2000 [01:10<09:47,  2.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 600: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 703/2000 [01:21<08:43,  2.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 700: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 803/2000 [01:32<08:18,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 800: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 903/2000 [01:43<07:29,  2.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 900: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1003/2000 [01:54<06:55,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1000: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 1103/2000 [02:05<06:08,  2.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1100: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 1203/2000 [02:15<05:28,  2.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1200: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 1303/2000 [02:26<04:36,  2.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1300: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 1403/2000 [02:37<04:00,  2.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1400: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 1503/2000 [02:48<03:33,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1500: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 1603/2000 [02:59<02:45,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1600: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 1703/2000 [03:09<01:58,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1700: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 1803/2000 [03:20<01:22,  2.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1800: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 1903/2000 [03:31<00:39,  2.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1900: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [03:42<00:00,  8.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1999: train loss inf, val loss inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan a trip to Parise verecay chexpfoyndway rixppsiaon UMef Enth Ps gas.\n",
            "Éalbu, Brky Bracrthy Lit me forren theavintr Pa\n"
          ]
        }
      ],
      "source": [
        "# Train with REINFORCE\n",
        "train_reinforce()\n",
        "\n",
        "# Generate travel itinerary\n",
        "context = torch.tensor(encode('Plan a trip to Paris'), dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated = model.generate(context, max_new_tokens=100, mode='policy')\n",
        "print(decode(generated[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- **Dataset**: The sample `travel.txt` is ~1,200 characters. For better results, use a larger dataset (e.g., 100,000+ characters from travel guides or blogs).\n",
        "- **Reward Function**: The sentiment-based reward can be replaced with a travel-specific metric (e.g., keyword matching for 'Eiffel Tower'). Example:\n",
        "  ```python\n",
        "  def compute_travel_reward(response):\n",
        "      keywords = ['Eiffel Tower', 'Louvre', 'Paris', 'museum', 'café']\n",
        "      return sum(1 for keyword in keywords if keyword.lower() in response.lower())\n",
        "  ```\n",
        "- **Tokenizer**: Uses character-level tokenization. For subword tokenization, integrate `GPT2Tokenizer`:\n",
        "  ```python\n",
        "  from transformers import GPT2Tokenizer\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "  encode = lambda s: tokenizer.encode(s, return_tensors='pt').squeeze().tolist()\n",
        "  decode = lambda l: tokenizer.decode(l)\n",
        "  vocab_size = tokenizer.vocab_size\n",
        "  ```\n",
        "- **Hyperparameters**: Adjust `max_iters` (e.g., to 5000) or `block_size` for larger datasets.\n",
        "- **Visualization**: To inspect attention weights, add:\n",
        "  ```python\n",
        "  import matplotlib.pyplot as plt\n",
        "  weights = model(context, mode='policy')[2]\n",
        "  plt.imshow(weights.cpu().detach().numpy(), cmap='hot')\n",
        "  plt.title('Attention Weights')\n",
        "  plt.show()\n",
        "  ```\n",
        "- If errors occur, reduce `block_size` or expand `travel.txt`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
